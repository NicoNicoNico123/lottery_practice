{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG8etmziR3x3"
      },
      "source": [
        "## Initialization chrome driver"
      ],
      "id": "GG8etmziR3x3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I need to manually install chromedriver in Google Colab because Colab does not come with it pre-installed. As Colab's default runtime was changed from Ubuntu 18.04 LTS to Ubuntu 20.04 LTS, and chromium-browser is no longer distributed outside of the snap package. So the old method of installing Chromium via APT no longer works. By manually downloading the Chrome and chromedriver binaries, we can get around this issue and still use Selenium with Chrome in Colab notebooks.\n",
        "\n",
        "Reference: https://dev.classmethod.jp/articles/google-colaboratory-use-selenium/"
      ],
      "metadata": {
        "id": "2jW7Z_UN3Ble"
      },
      "id": "2jW7Z_UN3Ble"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0644c05",
        "outputId": "cf6d99ee-4879-4491-ec2b-540e1e9b9e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connecting to security.\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [80.4 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,790 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,243 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,580 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,310 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,623 kB]\n",
            "Fetched 22.3 MB in 3s (6,426 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "6 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "--2025-09-06 07:14:41--  http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 185.125.190.81, 185.125.190.39, 185.125.190.36, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|185.125.190.81|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3708 (3.6K) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libu2f-udev_1.1.4-1_all.deb’\n",
            "\n",
            "libu2f-udev_1.1.4-1 100%[===================>]   3.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-06 07:14:43 (528 MB/s) - ‘libu2f-udev_1.1.4-1_all.deb’ saved [3708/3708]\n",
            "\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "(Reading database ... 122217 files and directories currently installed.)\n",
            "Preparing to unpack libu2f-udev_1.1.4-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.4-1) ...\n",
            "Setting up libu2f-udev (1.1.4-1) ...\n",
            "--2025-09-06 07:14:43--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 142.251.189.190, 142.251.189.91, 142.251.189.93, ...\n",
            "Connecting to dl.google.com (dl.google.com)|142.251.189.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120610648 (115M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 115.02M   328MB/s    in 0.4s    \n",
            "\n",
            "2025-09-06 07:14:44 (328 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [120610648/120610648]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 122221 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (140.0.7339.80-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "--2025-09-06 07:14:54--  https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 142.251.189.207, 108.177.121.207, 209.85.145.207, ...\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|142.251.189.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7407250 (7.1M) [application/zip]\n",
            "Saving to: ‘/tmp/chromedriver_linux64.zip’\n",
            "\n",
            "chromedriver_linux6 100%[===================>]   7.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-09-06 07:14:54 (69.8 MB/s) - ‘/tmp/chromedriver_linux64.zip’ saved [7407250/7407250]\n",
            "\n",
            "Archive:  /tmp/chromedriver_linux64.zip\n",
            "  inflating: /tmp/chromedriver       \n",
            "  inflating: /tmp/LICENSE.chromedriver  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "# 更新を実行\n",
        "sudo apt -y update\n",
        "\n",
        "# ダウンロードのために必要なパッケージをインストール\n",
        "sudo apt install -y wget curl unzip\n",
        "# 以下はChromeの依存パッケージ\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "\n",
        "# Chromeのインストール\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "\n",
        "# Chrome Driverのインストール\n",
        "CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`\n",
        "wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver\n",
        "mv /tmp/chromedriver /usr/local/bin/chromedriver"
      ],
      "id": "c0644c05"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VPY9_uWSFZj"
      },
      "source": [
        "### Install selenium"
      ],
      "id": "1VPY9_uWSFZj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFLEGaVxi4_4",
        "outputId": "49d8b6c5-cc0c-4bce-a797-9165c0e4b0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Collecting typing_extensions~=4.14.0 (from selenium)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.30.0->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver-manager"
      ],
      "id": "cFLEGaVxi4_4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K2CDGhWiYai"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n"
      ],
      "id": "1K2CDGhWiYai"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJavDmMSPjC"
      },
      "source": [
        "## Scraping the data and save to csv"
      ],
      "id": "hWJavDmMSPjC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b231c633",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "\n",
        "# Initialize the DataFrame\n",
        "df = pd.DataFrame(columns=[\"YEAR\", \"TIMES\", \"N1\", \"N2\", \"N3\", \"N4\", \"N5\", \"N6\", \"S1\"])\n",
        "\n",
        "# Iterate over the years\n",
        "for year in range(2003, 2025):\n",
        "    # Open the webpage\n",
        "    driver.get(f\"http://www.nfd.com.tw/house/year/{year}.htm\")\n",
        "\n",
        "    # Scrape the data\n",
        "    data = []\n",
        "    table = driver.find_element(By.XPATH, \"//table\")\n",
        "    for row in table.find_elements(By.XPATH, \".//tr\"):\n",
        "        cols = row.find_elements(By.XPATH, \".//td\")\n",
        "        # Ensure row has data before appending\n",
        "        if cols:\n",
        "            data.append([col.text for col in cols])\n",
        "\n",
        "    # Convert data to DataFrame and append to the main DataFrame\n",
        "    df_year = pd.DataFrame(data, columns=[\"YEAR\", \"TIMES\", \"N1\", \"N2\", \"N3\", \"N4\", \"N5\", \"N6\", \"S1\"])\n",
        "    # Corrected line: use pd.concat() instead of df.concat()\n",
        "    df = pd.concat([df, df_year], ignore_index=True)\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "# Save the DataFrame as CSV\n",
        "df.to_csv(\"data.csv\", index=False)"
      ],
      "id": "b231c633"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ForToDLbk3uo"
      },
      "source": [
        "## Import Module"
      ],
      "id": "ForToDLbk3uo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaZTY7EYmSYL"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import sqrt, sin, cos, pi, zeros\n",
        "from numpy.random import randn, rand, uniform, normal\n",
        "from scipy.linalg import hadamard\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, LSTM, Dropout, RepeatVector, TimeDistributed, Embedding, Reshape, Dot, Concatenate\n",
        "from tensorflow.keras.layers import GRU, SpatialDropout1D, Conv1D, GlobalMaxPooling1D,Multiply, Lambda, Softmax, Flatten, BatchNormalization, Bidirectional, dot, concatenate\n",
        "from tensorflow.keras.layers import AdditiveAttention, Attention\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "id": "qaZTY7EYmSYL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Mark6 Lottery history\n"
      ],
      "metadata": {
        "id": "8Bgq8as_589X"
      },
      "id": "8Bgq8as_589X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a59b2fa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# date cleaning\n",
        "df = df[df['YEAR'] != \"YEAR\"]\n",
        "df = df[df['YEAR'] != \"新冠疫情.七個月未開  2020 / 02 / 01~2020 / 09 / 24\"]\n",
        "df.to_csv('data.csv', index=False)\n",
        "df.tail()"
      ],
      "id": "2a59b2fa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU9DxxoN_pzy"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=[\"YEAR\", \"TIMES\"])"
      ],
      "id": "GU9DxxoN_pzy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v77SvGDUoUC1"
      },
      "outputs": [],
      "source": [
        "df = df.astype(int)"
      ],
      "id": "v77SvGDUoUC1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the times series data"
      ],
      "metadata": {
        "id": "XOuPs-twfa9T"
      },
      "id": "XOuPs-twfa9T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc5_81ZGjEhC"
      },
      "outputs": [],
      "source": [
        "data = df.values - 1\n",
        "train = data[:-50]\n",
        "test = data[-50:]\n",
        "\n",
        "w = 15\n",
        "X_train = []\n",
        "y_train = []\n",
        "for i in range(w, len(train)):\n",
        "    X_train.append(train[i - w: i, :])\n",
        "    y_train.append(train[i])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "inputs = data[data.shape[0] - test.shape[0] - w:]\n",
        "X_test = []\n",
        "for i in range(w, inputs.shape[0]):\n",
        "    X_test.append(inputs[i - w: i, :])\n",
        "X_test = np.array(X_test)\n",
        "y_test = test"
      ],
      "id": "Bc5_81ZGjEhC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiX8MTGFk5UZ"
      },
      "outputs": [],
      "source": [
        "print(data.shape)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "id": "kiX8MTGFk5UZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling setup"
      ],
      "metadata": {
        "id": "vOUgWw6afEn3"
      },
      "id": "vOUgWw6afEn3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOg930lRlqUG"
      },
      "outputs": [],
      "source": [
        "embed_dim = (59 // 2) + 1\n",
        "dropout_rate = 0.5\n",
        "spatial_dropout_rate = 0.5\n",
        "steps_before = w\n",
        "steps_after = 7\n",
        "feature_count = embed_dim * 7\n",
        "hidden_neurons = [64, 32]\n",
        "bidirectional = True\n",
        "attention_style = 'Bahdanau'"
      ],
      "id": "ZOg930lRlqUG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hea3g-V3lqMA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Create an instance of MirroredStrategy.\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "    inp0 = Input(shape = (w, X_train.shape[2]))\n",
        "\n",
        "    inp1 = Lambda(lambda x: x[:, :, 0])(inp0)\n",
        "    inp1 = Embedding(49, embed_dim)(inp1)\n",
        "    inp1 = SpatialDropout1D(spatial_dropout_rate)(inp1)\n",
        "\n",
        "    inp2 = Lambda(lambda x: x[:, :, 1])(inp0)\n",
        "    inp2 = Embedding(49, embed_dim)(inp2)\n",
        "    inp2 = SpatialDropout1D(spatial_dropout_rate)(inp2)\n",
        "\n",
        "    inp3 = Lambda(lambda x: x[:, :, 2])(inp0)\n",
        "    inp3 = Embedding(49, embed_dim)(inp3)\n",
        "    inp3 = SpatialDropout1D(spatial_dropout_rate)(inp3)\n",
        "\n",
        "    inp4 = Lambda(lambda x: x[:, :, 3])(inp0)\n",
        "    inp4 = Embedding(49, embed_dim)(inp4)\n",
        "    inp4 = SpatialDropout1D(spatial_dropout_rate)(inp4)\n",
        "\n",
        "    inp5 = Lambda(lambda x: x[:, :, 4])(inp0)\n",
        "    inp5 = Embedding(49, embed_dim)(inp5)\n",
        "    inp5 = SpatialDropout1D(spatial_dropout_rate)(inp5)\n",
        "\n",
        "    inp6 = Lambda(lambda x: x[:, :, 5])(inp0)\n",
        "    inp6 = Embedding(49, embed_dim)(inp6)\n",
        "    inp6 = SpatialDropout1D(spatial_dropout_rate)(inp6)\n",
        "\n",
        "    inp7 = Lambda(lambda x: x[:, :, 6])(inp0)\n",
        "    inp7 = Embedding(49, embed_dim)(inp7)\n",
        "    inp7 = SpatialDropout1D(spatial_dropout_rate)(inp7)\n",
        "\n",
        "    inp = Concatenate()([inp1, inp2, inp3, inp4, inp5, inp6, inp7])\n",
        "\n",
        "    # Seq2Seq model with attention or bidirectional encoder\n",
        "\n",
        "    num_layers = len(hidden_neurons)\n",
        "\n",
        "    sh_list, h_list, c_list = [inp], [], []\n",
        "\n",
        "    if bidirectional:\n",
        "\n",
        "        for i in range(num_layers):\n",
        "\n",
        "            sh, fh, fc, bh, bc = Bidirectional(LSTM(hidden_neurons[i],\n",
        "                                                    dropout = dropout_rate,\n",
        "                                                    return_state = True,\n",
        "                                                    return_sequences = True,\n",
        "                                                    kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "                                                    recurrent_regularizer=tf.keras.regularizers.l2(1e-4))\n",
        "\n",
        "                                                    )(sh_list[-1])\n",
        "\n",
        "            h = Concatenate()([fh, bh])\n",
        "            c = Concatenate()([fc, bc])\n",
        "\n",
        "            sh_list.append(sh)\n",
        "            h_list.append(h)\n",
        "            c_list.append(c)\n",
        "\n",
        "    else:\n",
        "\n",
        "        for i in range(num_layers):\n",
        "\n",
        "            sh, h, c = LSTM(hidden_neurons[i],\n",
        "                            dropout = dropout_rate,\n",
        "                            return_state = True,\n",
        "                            return_sequences = True,\n",
        "                            kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
        "                            recurrent_regularizer=tf.keras.regularizers.l2(1e-4)\n",
        "                            )(sh_list[-1])\n",
        "\n",
        "            sh_list.append(sh)\n",
        "            h_list.append(h)\n",
        "            c_list.append(c)\n",
        "\n",
        "    decoder = RepeatVector(steps_after)(h_list[-1])\n",
        "\n",
        "    if bidirectional:\n",
        "\n",
        "        decoder_hidden_neurons = [hn * 2 for hn in hidden_neurons]\n",
        "\n",
        "    else:\n",
        "\n",
        "        decoder_hidden_neurons = hidden_neurons\n",
        "\n",
        "    for i in range(num_layers):\n",
        "\n",
        "        decoder = LSTM(decoder_hidden_neurons[i],\n",
        "                       dropout = dropout_rate,\n",
        "                       return_sequences = True)(decoder, initial_state = [h_list[i], c_list[i]])\n",
        "\n",
        "    if attention_style == 'Bahdanau':\n",
        "\n",
        "        context = AdditiveAttention(dropout = dropout_rate)([decoder, sh_list[-1]])\n",
        "\n",
        "        decoder = concatenate([context, decoder])\n",
        "\n",
        "    elif attention_style == 'Luong':\n",
        "\n",
        "        context = Attention(dropout = dropout_rate)([decoder, sh_list[-1]])\n",
        "\n",
        "        decoder = concatenate([context, decoder])\n",
        "\n",
        "    out = Dense(49, activation = 'softmax')(decoder)\n",
        "\n",
        "    model = Model(inputs = inp0, outputs = out)\n",
        "\n",
        "    sparse_top_k = tf.keras.metrics.SparseTopKCategoricalAccuracy(k = 5, name = 'sparse_top_k')\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[sparse_top_k])"
      ],
      "id": "hea3g-V3lqMA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvGl1jWImpGb"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ],
      "id": "JvGl1jWImpGb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZyTgfTlm3e5"
      },
      "outputs": [],
      "source": [
        "class CosineAnnealingScheduler(callbacks.Callback):\n",
        "    \"\"\"Cosine annealing scheduler.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, T_max, eta_max, eta_min = 0, verbose = 0):\n",
        "        super(CosineAnnealingScheduler, self).__init__()\n",
        "        self.T_max = T_max\n",
        "        self.eta_max = eta_max\n",
        "        self.eta_min = eta_min\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs = None):\n",
        "        if not hasattr(self.model.optimizer, 'lr'):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
        "        backend.set_value(self.model.optimizer.lr, lr)\n",
        "        if self.verbose > 0:\n",
        "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
        "                  'rate to %s.' % (epoch + 1, lr))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs = None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = backend.get_value(self.model.optimizer.lr)"
      ],
      "id": "MZyTgfTlm3e5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Fit"
      ],
      "metadata": {
        "id": "Lzc_fsRYfNXI"
      },
      "id": "Lzc_fsRYfNXI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxw06Xdum88E"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 64\n",
        "LR_MAX = 1e-3\n",
        "LR_MIN = 1e-4\n",
        "\n",
        "cas = CosineAnnealingScheduler(EPOCHS, LR_MAX, LR_MIN)\n",
        "\n",
        "ckp = callbacks.ModelCheckpoint('best_model.hdf5', monitor = 'val_sparse_top_k', verbose = 0,\n",
        "                                save_best_only = True, save_weights_only = False, mode = 'max')\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data = (X_test, y_test),\n",
        "                    callbacks = [ckp, cas],\n",
        "                    epochs = EPOCHS,\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    verbose = 0)\n",
        "\n",
        "hist = pd.DataFrame(history.history)"
      ],
      "id": "Xxw06Xdum88E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl7R5WI2t317"
      },
      "outputs": [],
      "source": [
        "model.load_weights('best_model.hdf5')\n",
        "pred = model.predict(X_test)\n",
        "pred = np.argmax(pred, axis = 2)"
      ],
      "id": "Nl7R5WI2t317"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXJwHWj0W-8z"
      },
      "outputs": [],
      "source": [
        "loss_and_metrics = model.evaluate(X_test, y_test)\n",
        "loss_and_metrics"
      ],
      "id": "EXJwHWj0W-8z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJldRVTut7oA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training loss and sparse top k\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Over Epochs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['sparse_top_k'], label='Training Sparse Top K')\n",
        "plt.plot(history.history['val_sparse_top_k'], label='Validation Sparse Top K')\n",
        "plt.legend()\n",
        "plt.title('Sparse Top K Over Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "id": "uJldRVTut7oA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict of next draw"
      ],
      "metadata": {
        "id": "knEZXPOgWP6n"
      },
      "id": "knEZXPOgWP6n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgA6Nlzz16Cj"
      },
      "outputs": [],
      "source": [
        "X_latest = np.array([data[-w:, :]])\n",
        "pred_latest = model.predict(X_latest)\n",
        "pred_latest = np.squeeze(pred_latest)\n",
        "pred_latest_greedy = np.argmax(pred_latest, axis = 1)\n",
        "print(pred_latest_greedy + 1)"
      ],
      "id": "wgA6Nlzz16Cj"
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}